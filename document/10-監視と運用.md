# 10 - Áõ£Ë¶ñ„Å®ÈÅãÁî®

> [[09-„Çª„Ç≠„É•„É™„ÉÜ„Ç£„Å®„Éó„É©„Ç§„Éê„Ç∑„Éº]] | [[BondPointË®≠Ë®àÊõ∏]] | Next: [[11-ÈñãÁô∫„Çπ„Ç±„Ç∏„É•„Éº„É´]]

## üìä Áõ£Ë¶ñÊà¶Áï•

### Áõ£Ë¶ñ„ÅÆ4„Å§„ÅÆÊü±

```
üîç „É°„Éà„É™„ÇØ„Çπ     üìã „É≠„Ç∞
‚îú‚îÄ „Ç∑„Çπ„ÉÜ„É†      ‚îú‚îÄ „Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥
‚îú‚îÄ „Ç¢„Éó„É™        ‚îú‚îÄ „Çª„Ç≠„É•„É™„ÉÜ„Ç£
‚îú‚îÄ „Éì„Ç∏„Éç„Çπ      ‚îú‚îÄ Áõ£Êüª
‚îî‚îÄ „É¶„Éº„Ç∂„Éº      ‚îî‚îÄ „Ç®„É©„Éº

üìà „Éà„É¨„Éº„Çπ      üö® „Ç¢„É©„Éº„Éà
‚îú‚îÄ ÂàÜÊï£ËøΩË∑°      ‚îú‚îÄ „Åó„Åç„ÅÑÂÄ§
‚îú‚îÄ ‰æùÂ≠òÈñ¢‰øÇ      ‚îú‚îÄ Áï∞Â∏∏Ê§úÁü•
‚îú‚îÄ „Éú„Éà„É´„Éç„ÉÉ„ÇØ  ‚îú‚îÄ ÈÄöÁü•
‚îî‚îÄ „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ ‚îî‚îÄ „Ç®„Çπ„Ç´„É¨„Éº„Ç∑„Éß„É≥
```

---

## üìà „É°„Éà„É™„ÇØ„ÇπÁõ£Ë¶ñ

### „Ç∑„Çπ„ÉÜ„É†„É°„Éà„É™„ÇØ„Çπ

#### Cloud Run „É°„Éà„É™„ÇØ„Çπ

| „É°„Éà„É™„ÇØ„Çπ | „Åó„Åç„ÅÑÂÄ§ | „Ç¢„É©„Éº„Éà | ÂØæÂøú |
|------------|----------|----------|------|
| **CPU‰ΩøÁî®Áéá** | > 80% | Warning | „Ç™„Éº„Éà„Çπ„Ç±„Éº„É´Á¢∫Ë™ç |
| **„É°„É¢„É™‰ΩøÁî®Áéá** | > 85% | Warning | „É°„É¢„É™Âà∂ÈôêË¶ãÁõ¥„Åó |
| **„É™„ÇØ„Ç®„Çπ„ÉàÊï∞/Áßí** | > 1000 | Info | „Çπ„Ç±„Éº„É´Áä∂Ê≥ÅÁ¢∫Ë™ç |
| **„É¨„Çπ„Éù„É≥„ÇπÊôÇÈñì P95** | > 1Áßí | Critical | „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπË™øÊüª |
| **„Ç®„É©„ÉºÁéá** | > 1% | Critical | Á∑äÊÄ•ÂØæÂøú |
| **Ëµ∑ÂãïÊôÇÈñì** | > 10Áßí | Warning | „Ç≥„Éº„É´„Éâ„Çπ„Çø„Éº„ÉàÂØæÁ≠ñ |

#### Cloud SQL „É°„Éà„É™„ÇØ„Çπ

| „É°„Éà„É™„ÇØ„Çπ | „Åó„Åç„ÅÑÂÄ§ | „Ç¢„É©„Éº„Éà | ÂØæÂøú |
|------------|----------|----------|------|
| **Êé•Á∂öÊï∞** | > 80% | Warning | Êé•Á∂ö„Éó„Éº„É´Ë™øÊï¥ |
| **CPU‰ΩøÁî®Áéá** | > 80% | Warning | „Ç§„É≥„Çπ„Çø„É≥„ÇπÊã°Âºµ |
| **„Éá„Ç£„Çπ„ÇØ‰ΩøÁî®Áéá** | > 90% | Critical | „Çπ„Éà„É¨„Éº„Ç∏Êã°Âºµ |
| **„É¨„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥ÈÅÖÂª∂** | > 5Áßí | Warning | „É¨„Éó„É™„Ç´Á¢∫Ë™ç |
| **„Çπ„É≠„Éº„ÇØ„Ç®„É™** | > 1Áßí | Warning | „ÇØ„Ç®„É™ÊúÄÈÅ©Âåñ |

### „Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„É°„Éà„É™„ÇØ„Çπ

#### „Éì„Ç∏„Éç„Çπ„É°„Éà„É™„ÇØ„Çπ

```python
from prometheus_client import Counter, Histogram, Gauge
import time

# „Ç´„Çπ„Çø„É†„É°„Éà„É™„ÇØ„ÇπÂÆöÁæ©
status_updates_total = Counter(
    'bondpoint_status_updates_total',
    'Total number of status updates',
    ['status_type', 'user_id']
)

status_update_duration = Histogram(
    'bondpoint_status_update_duration_seconds',
    'Time spent processing status updates'
)

active_users = Gauge(
    'bondpoint_active_users',
    'Number of currently active users'
)

firestore_sync_errors = Counter(
    'bondpoint_firestore_sync_errors_total',
    'Total number of Firestore sync errors'
)

class MetricsCollector:
    """„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„É°„Éà„É™„ÇØ„ÇπÂèéÈõÜ"""

    @staticmethod
    def record_status_update(status_type: str, user_id: str, duration: float):
        """„Çπ„ÉÜ„Éº„Çø„ÇπÊõ¥Êñ∞„É°„Éà„É™„ÇØ„Çπ"""
        status_updates_total.labels(
            status_type=status_type,
            user_id=user_id
        ).inc()

        status_update_duration.observe(duration)

    @staticmethod
    def update_active_users(count: int):
        """„Ç¢„ÇØ„ÉÜ„Ç£„Éñ„É¶„Éº„Ç∂„ÉºÊï∞Êõ¥Êñ∞"""
        active_users.set(count)

    @staticmethod
    def record_sync_error(error_type: str):
        """ÂêåÊúü„Ç®„É©„ÉºË®òÈå≤"""
        firestore_sync_errors.labels(error_type=error_type).inc()

# ‰ΩøÁî®‰æã
async def update_user_status(event_id: str, user_id: str, status_data: dict):
    start_time = time.time()

    try:
        # „Çπ„ÉÜ„Éº„Çø„ÇπÊõ¥Êñ∞Âá¶ÁêÜ
        result = await process_status_update(event_id, user_id, status_data)

        # „É°„Éà„É™„ÇØ„ÇπË®òÈå≤
        duration = time.time() - start_time
        MetricsCollector.record_status_update(
            status_data['status_type'],
            user_id,
            duration
        )

        return result

    except FirestoreSyncError as e:
        MetricsCollector.record_sync_error("firestore_sync")
        raise
```

### „É¶„Éº„Ç∂„Éº„Ç®„ÇØ„Çπ„Éö„É™„Ç®„É≥„Çπ„É°„Éà„É™„ÇØ„Çπ

#### React Native „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ

```typescript
import analytics from '@react-native-firebase/analytics';
import perf from '@react-native-firebase/perf';

class UXMetrics {
  /**
   * ÁîªÈù¢Ë™≠„ÅøËæº„ÅøÊôÇÈñìÊ∏¨ÂÆö
   */
  static async measureScreenLoad(screenName: string) {
    const trace = perf().newTrace(`screen_load_${screenName}`);
    await trace.start();

    // ÁîªÈù¢Ë™≠„ÅøËæº„ÅøÂÆå‰∫ÜÊôÇ
    await trace.stop();
  }

  /**
   * „Çπ„ÉÜ„Éº„Çø„ÇπÊõ¥Êñ∞„É¨„Çπ„Éù„É≥„ÇπÊôÇÈñì
   */
  static async measureStatusUpdate(statusType: string) {
    const startTime = Date.now();

    try {
      await updateStatus(statusType);

      const duration = Date.now() - startTime;

      // Firebase Analytics „Å´Ë®òÈå≤
      await analytics().logEvent('status_update_performance', {
        status_type: statusType,
        duration_ms: duration,
        success: true
      });

    } catch (error) {
      const duration = Date.now() - startTime;

      await analytics().logEvent('status_update_performance', {
        status_type: statusType,
        duration_ms: duration,
        success: false,
        error_type: error.type
      });
    }
  }

  /**
   * „Ç¢„Éó„É™„ÇØ„É©„ÉÉ„Ç∑„É•ËøΩË∑°
   */
  static reportCrash(error: Error, screen: string) {
    analytics().logEvent('app_crash', {
      error_message: error.message,
      screen_name: screen,
      timestamp: Date.now()
    });
  }
}
```

---

## üìã „É≠„Ç∞ÁÆ°ÁêÜ

### ÊßãÈÄ†Âåñ„É≠„Ç∞

#### „É≠„Ç∞„É¨„Éô„É´ÂÆöÁæ©

| „É¨„Éô„É´ | Áî®ÈÄî | ‰æã | ‰øùÊåÅÊúüÈñì |
|--------|------|----|---------|
| **DEBUG** | ÈñãÁô∫ÊôÇË©≥Á¥∞ÊÉÖÂ†± | Èñ¢Êï∞Âëº„Å≥Âá∫„Åó„ÄÅÂ§âÊï∞ÂÄ§ | 7Êó• |
| **INFO** | ‰∏ÄËà¨ÁöÑ„Å™ÊÉÖÂ†± | APIÂëº„Å≥Âá∫„Åó„ÄÅ„Çπ„ÉÜ„Éº„Çø„ÇπÂ§âÊõ¥ | 30Êó• |
| **WARNING** | Ê≥®ÊÑè„ÅåÂøÖË¶Å„Å™Áä∂Ê≥Å | ÈÅÖÂª∂„ÄÅ„É™„Éà„É©„Ç§ | 90Êó• |
| **ERROR** | „Ç®„É©„ÉºÁô∫Áîü | ‰æãÂ§ñ„ÄÅÂ§±Êïó | 1Âπ¥ |
| **CRITICAL** | „Ç∑„Çπ„ÉÜ„É†ÂÅúÊ≠¢„É¨„Éô„É´ | „Çµ„Éº„Éì„ÇπÂÅúÊ≠¢„ÄÅ„Éá„Éº„ÇøÁ†¥Êêç | Ê∞∏Á∂ö |

#### „É≠„Ç∞ÂΩ¢ÂºèÊ®ôÊ∫ñÂåñ

```python
import logging
import json
from datetime import datetime
from typing import Optional

class StructuredLogger:
    """ÊßãÈÄ†Âåñ„É≠„Ç∞Âá∫Âäõ"""

    def __init__(self, service_name: str):
        self.service_name = service_name
        self.logger = logging.getLogger(service_name)

    def log(
        self,
        level: str,
        message: str,
        user_id: Optional[str] = None,
        event_id: Optional[str] = None,
        trace_id: Optional[str] = None,
        **kwargs
    ):
        """ÊßãÈÄ†Âåñ„É≠„Ç∞Âá∫Âäõ"""
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "severity": level.upper(),
            "service": self.service_name,
            "message": message,
            "trace_id": trace_id,
            "user_id": user_id,
            "event_id": event_id,
            **kwargs
        }

        # JSONÂΩ¢Âºè„ÅßÂá∫ÂäõÔºàCloud LoggingÂêë„ÅëÔºâ
        self.logger.log(
            getattr(logging, level.upper()),
            json.dumps(log_entry, ensure_ascii=False)
        )

    def info_status_update(
        self,
        user_id: str,
        event_id: str,
        old_status: str,
        new_status: str,
        trace_id: str
    ):
        """„Çπ„ÉÜ„Éº„Çø„ÇπÊõ¥Êñ∞„É≠„Ç∞"""
        self.log(
            "info",
            "User status updated",
            user_id=user_id,
            event_id=event_id,
            trace_id=trace_id,
            old_status=old_status,
            new_status=new_status,
            operation="status_update"
        )

    def error_firestore_sync(
        self,
        user_id: str,
        event_id: str,
        error: Exception,
        trace_id: str
    ):
        """FirestoreÂêåÊúü„Ç®„É©„Éº„É≠„Ç∞"""
        self.log(
            "error",
            "Firestore sync failed",
            user_id=user_id,
            event_id=event_id,
            trace_id=trace_id,
            error_type=type(error).__name__,
            error_message=str(error),
            operation="firestore_sync"
        )

# ‰ΩøÁî®‰æã
logger = StructuredLogger("bondpoint-api")

async def update_user_status(event_id: str, user_id: str, status_data: dict):
    trace_id = generate_trace_id()

    try:
        old_status = await get_current_status(event_id, user_id)
        new_status = await save_status(event_id, user_id, status_data)

        # ÊàêÂäü„É≠„Ç∞
        logger.info_status_update(
            user_id, event_id,
            old_status.status_type if old_status else None,
            new_status.status_type,
            trace_id
        )

        return new_status

    except Exception as e:
        # „Ç®„É©„Éº„É≠„Ç∞
        logger.error_firestore_sync(user_id, event_id, e, trace_id)
        raise
```

### „É≠„Ç∞Ê§úÁ¥¢„ÉªÂàÜÊûê

#### BigQuery „É≠„Ç∞ÂàÜÊûê

```sql
-- „Çπ„ÉÜ„Éº„Çø„ÇπÊõ¥Êñ∞„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂàÜÊûê
SELECT
  JSON_EXTRACT_SCALAR(jsonPayload, '$.user_id') as user_id,
  JSON_EXTRACT_SCALAR(jsonPayload, '$.old_status') as old_status,
  JSON_EXTRACT_SCALAR(jsonPayload, '$.new_status') as new_status,
  TIMESTAMP_DIFF(
    timestamp,
    LAG(timestamp) OVER (
      PARTITION BY JSON_EXTRACT_SCALAR(jsonPayload, '$.user_id')
      ORDER BY timestamp
    ),
    MILLISECOND
  ) as update_interval_ms
FROM `bondpoint.logs.cloudrun`
WHERE
  DATE(timestamp) = CURRENT_DATE()
  AND JSON_EXTRACT_SCALAR(jsonPayload, '$.operation') = 'status_update'
  AND severity = 'INFO'
ORDER BY timestamp DESC;

-- „Ç®„É©„ÉºÁéáÂàÜÊûê
SELECT
  DATE(timestamp) as date,
  COUNT(*) as total_requests,
  COUNTIF(severity = 'ERROR') as error_count,
  ROUND(COUNTIF(severity = 'ERROR') / COUNT(*) * 100, 2) as error_rate_percent
FROM `bondpoint.logs.cloudrun`
WHERE
  DATE(timestamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
GROUP BY date
ORDER BY date DESC;

-- „É¶„Éº„Ç∂„ÉºË°åÂãïÂàÜÊûê
SELECT
  JSON_EXTRACT_SCALAR(jsonPayload, '$.new_status') as status_type,
  COUNT(*) as update_count,
  COUNT(DISTINCT JSON_EXTRACT_SCALAR(jsonPayload, '$.user_id')) as unique_users,
  ROUND(AVG(TIMESTAMP_DIFF(
    LEAD(timestamp) OVER (
      PARTITION BY JSON_EXTRACT_SCALAR(jsonPayload, '$.user_id')
      ORDER BY timestamp
    ),
    timestamp,
    SECOND
  )), 2) as avg_duration_seconds
FROM `bondpoint.logs.cloudrun`
WHERE
  DATE(timestamp) = CURRENT_DATE()
  AND JSON_EXTRACT_SCALAR(jsonPayload, '$.operation') = 'status_update'
GROUP BY status_type
ORDER BY update_count DESC;
```

---

## üö® „Ç¢„É©„Éº„Éà„Ç∑„Çπ„ÉÜ„É†

### „Ç¢„É©„Éº„ÉàÈöéÂ±§

#### „É¨„Éô„É´Âà•ÂØæÂøú

| „É¨„Éô„É´ | ÈÄöÁü•ÂÖà | ÂØæÂøúÊôÇÈñì | „Ç®„Çπ„Ç´„É¨„Éº„Ç∑„Éß„É≥ |
|--------|--------|----------|------------------|
| **Info** | Slack | - | „Å™„Åó |
| **Warning** | Slack + Email | 30ÂàÜ | 1ÊôÇÈñìÂæå„Å´„Ç®„Çπ„Ç´„É¨„Éº„Éà |
| **Critical** | Slack + Email + SMS | 5ÂàÜ | 15ÂàÜÂæå„Å´„Ç®„Çπ„Ç´„É¨„Éº„Éà |
| **Emergency** | ÂÖ®„ÉÅ„É£„Éç„É´ + ÈõªË©± | Âç≥Â∫ß | 5ÂàÜÂæå„Å´„Éû„Éç„Éº„Ç∏„É£„Éº |

#### „Ç¢„É©„Éº„Éà„É´„Éº„É´

```yaml
# Cloud Monitoring „Ç¢„É©„Éº„Éà„Éù„É™„Ç∑„Éº
api_error_rate:
  condition:
    metric: "compute.googleapis.com/instance/cpu/utilization"
    filter: 'resource.type="gce_instance"'
    threshold: 0.8
    duration: 300s
  notification:
    - slack-channel: "#alerts"
    - email: "dev-team@bondpoint.com"

status_update_latency:
  condition:
    metric: "custom.googleapis.com/status_update_duration"
    aggregation: "95th percentile"
    threshold: 2.0
    duration: 180s
  notification:
    - slack-channel: "#performance"

firestore_sync_errors:
  condition:
    metric: "custom.googleapis.com/firestore_sync_errors"
    rate: "> 10/minute"
    duration: 60s
  severity: "critical"
  notification:
    - slack-channel: "#alerts"
    - email: "oncall@bondpoint.com"
    - sms: "+81-80-1234-5678"
```

### Áï∞Â∏∏Ê§úÁü•

#### Ê©üÊ¢∞Â≠¶Áøí„Éô„Éº„ÇπÊ§úÁü•

```python
from sklearn.ensemble import IsolationForest
import numpy as np

class AnomalyDetector:
    """Áï∞Â∏∏Ê§úÁü•„Ç∑„Çπ„ÉÜ„É†"""

    def __init__(self):
        self.model = IsolationForest(contamination=0.1)
        self.is_trained = False

    def train(self, normal_data: np.ndarray):
        """Ê≠£Â∏∏„Éá„Éº„Çø„Åß„É¢„Éá„É´Ë®ìÁ∑¥"""
        self.model.fit(normal_data)
        self.is_trained = True

    def detect_anomaly(self, metrics: dict) -> bool:
        """Áï∞Â∏∏Ê§úÁü•"""
        if not self.is_trained:
            return False

        # „É°„Éà„É™„ÇØ„Çπ„ÇíÁâπÂæ¥Èáè„Éô„ÇØ„Éà„É´„Å´Â§âÊèõ
        features = np.array([
            metrics['response_time'],
            metrics['error_rate'],
            metrics['request_count'],
            metrics['cpu_usage'],
            metrics['memory_usage']
        ]).reshape(1, -1)

        # Áï∞Â∏∏„Çπ„Ç≥„Ç¢Ë®àÁÆó
        anomaly_score = self.model.decision_function(features)[0]

        # „Åó„Åç„ÅÑÂÄ§„Å´„Çà„ÇãÂà§ÂÆö
        return anomaly_score < -0.5

# ‰ΩøÁî®‰æã
anomaly_detector = AnomalyDetector()

async def check_system_health():
    """„Ç∑„Çπ„ÉÜ„É†„Éò„É´„Çπ„ÉÅ„Çß„ÉÉ„ÇØ"""
    current_metrics = {
        'response_time': await get_avg_response_time(),
        'error_rate': await get_error_rate(),
        'request_count': await get_request_count(),
        'cpu_usage': await get_cpu_usage(),
        'memory_usage': await get_memory_usage()
    }

    if anomaly_detector.detect_anomaly(current_metrics):
        await send_anomaly_alert(current_metrics)
```

---

## üõ†Ô∏è ÈÅãÁî®„Éó„É≠„Çª„Çπ

### „Ç§„É≥„Ç∑„Éá„É≥„ÉàÂØæÂøú

#### „Ç§„É≥„Ç∑„Éá„É≥„ÉàÂàÜÈ°û

| ÈáçË¶ÅÂ∫¶ | ÂÆöÁæ© | ÁõÆÊ®ôÂØæÂøúÊôÇÈñì | ‰æã |
|--------|------|-------------|-----|
| **P0** | „Çµ„Éº„Éì„ÇπÂÖ®ÂÅúÊ≠¢ | 15ÂàÜ | APIÂÆåÂÖ®ÂÅúÊ≠¢„ÄÅ„Éá„Éº„ÇøÊ∂àÂ§± |
| **P1** | ‰∏ªË¶ÅÊ©üËÉΩÂÅúÊ≠¢ | 1ÊôÇÈñì | „Çπ„ÉÜ„Éº„Çø„ÇπÊõ¥Êñ∞‰∏çÂèØ |
| **P2** | ‰∏ÄÈÉ®Ê©üËÉΩÂΩ±Èüø | 4ÊôÇÈñì | ‰ΩçÁΩÆÊÉÖÂ†±Ë°®Á§∫Áï∞Â∏∏ |
| **P3** | ËªΩÂæÆ„Å™ÂïèÈ°å | 24ÊôÇÈñì | UIË°®Á§∫Â¥©„Çå |

#### ÂØæÂøú„Éï„É≠„Éº

```
üîç Ê§úÁü•
‚îú‚îÄ Ëá™ÂãïÁõ£Ë¶ñ„Ç∑„Çπ„ÉÜ„É†
‚îú‚îÄ „É¶„Éº„Ç∂„ÉºÂ†±Âëä
‚îî‚îÄ ÂÆöÊúü„Éò„É´„Çπ„ÉÅ„Çß„ÉÉ„ÇØ
    ‚îÇ
    ‚Üì
üö® „Éà„É™„Ç¢„Éº„Ç∏
‚îú‚îÄ ÈáçË¶ÅÂ∫¶Âà§ÂÆö
‚îú‚îÄ ÂΩ±ÈüøÁØÑÂõ≤Á¢∫Ë™ç
‚îî‚îÄ ÊãÖÂΩìËÄÖ„Ç¢„Çµ„Ç§„É≥
    ‚îÇ
    ‚Üì
üîß ÂØæÂøú
‚îú‚îÄ ÂøúÊÄ•Âá¶ÁΩÆ
‚îú‚îÄ Ê†πÊú¨ÂéüÂõ†Ë™øÊüª
‚îî‚îÄ ‰øÆÊ≠£ÂÆüË£Ö
    ‚îÇ
    ‚Üì
‚úÖ Ëß£Ê±∫
‚îú‚îÄ Ê©üËÉΩÂõûÂæ©Á¢∫Ë™ç
‚îú‚îÄ „É¶„Éº„Ç∂„ÉºÈÄöÁü•
‚îî‚îÄ „Éù„Çπ„Éà„É¢„Éº„ÉÜ„É†‰ΩúÊàê
```

#### „Ç§„É≥„Ç∑„Éá„É≥„ÉàÁÆ°ÁêÜ

```python
from enum import Enum
from dataclasses import dataclass
from datetime import datetime
from typing import List, Optional

class IncidentSeverity(Enum):
    P0 = "critical"
    P1 = "high"
    P2 = "medium"
    P3 = "low"

class IncidentStatus(Enum):
    OPEN = "open"
    INVESTIGATING = "investigating"
    IDENTIFIED = "identified"
    MONITORING = "monitoring"
    RESOLVED = "resolved"

@dataclass
class Incident:
    id: str
    title: str
    description: str
    severity: IncidentSeverity
    status: IncidentStatus
    created_at: datetime
    updated_at: datetime
    assigned_to: Optional[str] = None
    tags: List[str] = None
    affected_users: int = 0

class IncidentManager:
    """„Ç§„É≥„Ç∑„Éá„É≥„ÉàÁÆ°ÁêÜ"""

    @staticmethod
    async def create_incident(
        title: str,
        description: str,
        severity: IncidentSeverity,
        tags: List[str] = None
    ) -> Incident:
        """„Ç§„É≥„Ç∑„Éá„É≥„Éà‰ΩúÊàê"""
        incident = Incident(
            id=generate_incident_id(),
            title=title,
            description=description,
            severity=severity,
            status=IncidentStatus.OPEN,
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow(),
            tags=tags or []
        )

        await save_incident(incident)

        # ÈáçË¶ÅÂ∫¶„Å´Âøú„Åò„ÅüÈÄöÁü•
        if severity in [IncidentSeverity.P0, IncidentSeverity.P1]:
            await send_critical_alert(incident)

        return incident

    @staticmethod
    async def update_incident_status(
        incident_id: str,
        new_status: IncidentStatus,
        update_message: str
    ):
        """„Ç§„É≥„Ç∑„Éá„É≥„ÉàÁä∂Ê≥ÅÊõ¥Êñ∞"""
        incident = await get_incident(incident_id)
        incident.status = new_status
        incident.updated_at = datetime.utcnow()

        await save_incident(incident)
        await post_status_update(incident, update_message)

        # Ëß£Ê±∫ÊôÇ„ÅÆÂá¶ÁêÜ
        if new_status == IncidentStatus.RESOLVED:
            await schedule_postmortem(incident)
```

### „Éá„Éó„É≠„Ç§„É°„É≥„Éà

#### Blue-Green „Éá„Éó„É≠„Ç§„É°„É≥„Éà

```yaml
# Cloud Run Blue-Green „Éá„Éó„É≠„Ç§
steps:
  # 1. GreenÁí∞Â¢É„Å´„Éá„Éó„É≠„Ç§
  - name: 'gcr.io/cloud-builders/gcloud'
    args:
      - 'run'
      - 'deploy'
      - 'bondpoint-api-green'
      - '--image'
      - 'gcr.io/$PROJECT_ID/bondpoint-api:$COMMIT_SHA'
      - '--region'
      - 'asia-northeast1'
      - '--no-traffic'

  # 2. „Éò„É´„Çπ„ÉÅ„Çß„ÉÉ„ÇØ
  - name: 'gcr.io/cloud-builders/curl'
    args:
      - '-f'
      - 'https://bondpoint-api-green-xyz.a.run.app/health'

  # 3. „Çπ„É¢„Éº„ÇØ„ÉÜ„Çπ„Éà
  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        # APIÂãï‰ΩúÁ¢∫Ë™ç
        response=$(curl -s https://bondpoint-api-green-xyz.a.run.app/api/v1/health)
        if [[ "$response" != *"healthy"* ]]; then
          echo "Health check failed"
          exit 1
        fi

  # 4. „Éà„É©„Éï„Ç£„ÉÉ„ÇØÂàá„ÇäÊõø„ÅàÔºàÊÆµÈöéÁöÑÔºâ
  - name: 'gcr.io/cloud-builders/gcloud'
    args:
      - 'run'
      - 'services'
      - 'update-traffic'
      - 'bondpoint-api'
      - '--to-revisions'
      - 'bondpoint-api-green=10'
      - '--region'
      - 'asia-northeast1'

  # 5. Áõ£Ë¶ñÊúüÈñìÔºà5ÂàÜÔºâ
  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        sleep 300
        # „Ç®„É©„ÉºÁéá„ÉÅ„Çß„ÉÉ„ÇØ
        error_rate=$(gcloud monitoring metrics list --filter="metric.type=custom.googleapis.com/error_rate" --format="value(points[0].value.doubleValue)")
        if (( $(echo "$error_rate > 0.01" | bc -l) )); then
          echo "Error rate too high: $error_rate"
          # „É≠„Éº„É´„Éê„ÉÉ„ÇØ
          gcloud run services update-traffic bondpoint-api --to-revisions bondpoint-api-blue=100
          exit 1
        fi

  # 6. ÂÆåÂÖ®Âàá„ÇäÊõø„Åà
  - name: 'gcr.io/cloud-builders/gcloud'
    args:
      - 'run'
      - 'services'
      - 'update-traffic'
      - 'bondpoint-api'
      - '--to-revisions'
      - 'bondpoint-api-green=100'
```

### „Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó„ÉªÂæ©Êóß

#### Ëá™Âãï„Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó

```bash
#!/bin/bash
# daily_backup.sh

set -e

# Â§âÊï∞Ë®≠ÂÆö
PROJECT_ID="bondpoint-prod"
DB_INSTANCE="bondpoint-db"
BUCKET="bondpoint-backups"
DATE=$(date +%Y%m%d_%H%M%S)

echo "Starting backup at $(date)"

# 1. Cloud SQL „Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó
echo "Creating Cloud SQL backup..."
gcloud sql backups create \
  --instance=$DB_INSTANCE \
  --description="Daily backup $DATE" \
  --project=$PROJECT_ID

# 2. Firestore „Ç®„ÇØ„Çπ„Éù„Éº„Éà
echo "Exporting Firestore data..."
gcloud firestore export gs://$BUCKET/firestore/$DATE \
  --project=$PROJECT_ID

# 3. Cloud Storage „Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó
echo "Backing up user data..."
gsutil -m cp -r gs://bondpoint-user-images gs://$BUCKET/user-images/$DATE/

# 4. Secret Manager „Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó
echo "Backing up secrets metadata..."
gcloud secrets list --format="value(name)" | while read secret; do
  gcloud secrets describe $secret --format=json > /tmp/${secret}.json
done
gsutil cp /tmp/*.json gs://$BUCKET/secrets/$DATE/

# 5. „Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„ÉóÊ§úË®º
echo "Verifying backups..."
gsutil ls gs://$BUCKET/firestore/$DATE/ > /dev/null
gsutil ls gs://$BUCKET/user-images/$DATE/ > /dev/null

# 6. Âè§„ÅÑ„Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„ÉóÂâäÈô§Ôºà30Êó•‰ª•‰∏äÔºâ
echo "Cleaning up old backups..."
gsutil -m rm -r gs://$BUCKET/firestore/$(date -d '30 days ago' +%Y%m%d)_* || true
gsutil -m rm -r gs://$BUCKET/user-images/$(date -d '30 days ago' +%Y%m%d)_* || true

echo "Backup completed successfully at $(date)"

# 7. ÁµêÊûúÈÄöÁü•
curl -X POST https://hooks.slack.com/services/... \
  -H 'Content-type: application/json' \
  --data "{\"text\":\"‚úÖ Daily backup completed: $DATE\"}"
```

---

## üîß ‰øùÂÆà„ÉªÈÅãÁî®ÊâãÈ†Ü

### ÂÆöÊúü„É°„É≥„ÉÜ„Éä„É≥„Çπ

#### ÊúàÊ¨°„É°„É≥„ÉÜ„Éä„É≥„Çπ

```bash
#!/bin/bash
# monthly_maintenance.sh

echo "=== Monthly Maintenance $(date) ==="

# 1. ‰æùÂ≠òÈñ¢‰øÇÊõ¥Êñ∞
echo "Updating dependencies..."
pip-review --auto
npm audit fix

# 2. „Çª„Ç≠„É•„É™„ÉÜ„Ç£„Éë„ÉÉ„ÉÅÈÅ©Áî®
echo "Applying security patches..."
gcloud sql patches apply --patch=... --instance=bondpoint-db

# 3. „Éá„Éº„Çø„Éô„Éº„ÇπÊúÄÈÅ©Âåñ
echo "Optimizing database..."
psql $DATABASE_URL << EOF
VACUUM ANALYZE;
REINDEX DATABASE bondpoint;
EOF

# 4. Âè§„ÅÑ„Éá„Éº„ÇøÂâäÈô§
echo "Cleaning up old data..."
python scripts/cleanup_old_data.py

# 5. „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÉÜ„Çπ„Éà
echo "Running performance tests..."
locust --headless -u 100 -r 10 -t 300s --host=https://api.bondpoint.com

# 6. „Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó„ÉÜ„Çπ„Éà
echo "Testing backup restoration..."
./scripts/test_backup_restore.sh

# 7. „Çª„Ç≠„É•„É™„ÉÜ„Ç£„Çπ„Ç≠„É£„É≥
echo "Running security scan..."
nmap -sV api.bondpoint.com
bandit -r app/

echo "=== Maintenance completed ==="
```

### „Ç≠„É£„Éë„Ç∑„ÉÜ„Ç£„Éó„É©„É≥„Éã„É≥„Ç∞

#### ÊàêÈï∑‰∫àÊ∏¨

```python
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

class CapacityPlanner:
    """„Ç≠„É£„Éë„Ç∑„ÉÜ„Ç£Ë®àÁîª"""

    def __init__(self):
        self.user_growth_model = LinearRegression()
        self.load_prediction_model = LinearRegression()

    def analyze_growth_trend(self, historical_data: pd.DataFrame):
        """ÊàêÈï∑„Éà„É¨„É≥„ÉâÂàÜÊûê"""
        # „É¶„Éº„Ç∂„ÉºÊï∞„ÅÆÊàêÈï∑‰∫àÊ∏¨
        X = historical_data[['days_since_launch']].values
        y = historical_data['daily_active_users'].values

        self.user_growth_model.fit(X, y)

        # 3„É∂ÊúàÂæå„ÅÆ‰∫àÊ∏¨
        future_days = np.array([[90]])  # 90Êó•Âæå
        predicted_users = self.user_growth_model.predict(future_days)[0]

        return {
            'predicted_dau_3months': int(predicted_users),
            'growth_rate_per_day': self.user_growth_model.coef_[0],
            'confidence_score': self.user_growth_model.score(X, y)
        }

    def recommend_scaling(self, current_metrics: dict, predicted_load: dict):
        """„Çπ„Ç±„Éº„É™„É≥„Ç∞Êé®Â•®"""
        recommendations = []

        # Cloud Run Êé®Â•®
        if predicted_load['requests_per_second'] > 500:
            recommendations.append({
                'service': 'Cloud Run',
                'action': 'Increase max instances to 200',
                'reason': 'High request volume predicted'
            })

        # Cloud SQL Êé®Â•®
        if predicted_load['db_connections'] > current_metrics['max_connections'] * 0.8:
            recommendations.append({
                'service': 'Cloud SQL',
                'action': 'Upgrade to db-n1-standard-2',
                'reason': 'Connection pool will be exhausted'
            })

        return recommendations
```

---

**Next**: [[11-ÈñãÁô∫„Çπ„Ç±„Ç∏„É•„Éº„É´]] - 12ÈÄ±Èñì„ÅÆË©≥Á¥∞ÂÆüË£ÖË®àÁîª